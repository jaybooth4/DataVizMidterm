{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsTrain = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), download_if_missing=True)\n",
    "newsTest = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), download_if_missing=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Example BOW Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import download\n",
    "download('stopwords')\n",
    "download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def filterWords(word):\n",
    "    return word not in stop and word.isalpha()\n",
    "\n",
    "def customTokenizer(doc):\n",
    "    words = [word for sentence in sent_tokenize(doc) for word in word_tokenize(sentence)]\n",
    "    return(list(filter(lambda word: filterWords(word), words)))\n",
    "\n",
    "nltkCountVectorizer = CountVectorizer(tokenizer=customTokenizer, ngram_range=(1,1), max_df=.5, min_df=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from gensim.matutils import Sparse2Corpus\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def preprocessData(vectorizer, data):\n",
    "    bow = vectorizer.fit_transform(data)\n",
    "    tfidf = TfidfTransformer().fit_transform(bow)\n",
    "    id2word = dict((id, word) for word, id in nltkCountVectorizer.vocabulary_.items())\n",
    "    return bow, tfidf, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainLDA(docRep, id2word):\n",
    "    ldamodel = LdaModel(Sparse2Corpus(docRep), num_topics=20, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow, tfidf, id2word = preprocessData(nltkCountVectorizer, newsTrain[:10])\n",
    "ldaModelBOW = trainLDA(bow, id2word)\n",
    "ldaModelTFIDF = trainLDA(bow, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "clusterData = []\n",
    "maxLength = max(map(lambda doc: doc[-1][0], bow))\n",
    "for doc in bow:\n",
    "    docDict = dict(doc)\n",
    "    vectorOnly = []\n",
    "    for i in range(maxLength):\n",
    "        vectorOnly.append(docDict.get(i, 0))\n",
    "    clusterData.append(vectorOnly)\n",
    "\n",
    "kmeans = KMeans(n_clusters=20).fit(clusterData)\n",
    "# kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "# Similarity metric between groups\n",
    "score = adjusted_rand_score(kmeans.labels_, corpusTopics[:NUM_DOCS])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(newsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'sklearn.feature_extraction.text.CountVectorizer'>\n",
      "['__abs__', '__add__', '__array_priority__', '__bool__', '__class__', '__delattr__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__idiv__', '__imul__', '__init__', '__init_subclass__', '__isub__', '__iter__', '__itruediv__', '__le__', '__len__', '__lt__', '__matmul__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__rmatmul__', '__rmul__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '_add_dense', '_add_sparse', '_arg_min_or_max', '_arg_min_or_max_axis', '_binopt', '_boolean_index_to_array', '_check_boolean', '_check_ellipsis', '_cs_matrix__get_has_canonical_format', '_cs_matrix__get_sorted', '_cs_matrix__set_has_canonical_format', '_cs_matrix__set_sorted', '_deduped_data', '_divide', '_divide_sparse', '_get_dtype', '_get_row_slice', '_get_single_element', '_get_submatrix', '_imag', '_index_to_arrays', '_inequality', '_insert_many', '_maximum_minimum', '_min_or_max', '_min_or_max_axis', '_minor_reduce', '_mul_multivector', '_mul_scalar', '_mul_sparse_matrix', '_mul_vector', '_prepare_indices', '_process_toarray_args', '_real', '_rsub_dense', '_scalar_binopt', '_set_dtype', '_set_many', '_set_self', '_setdiag', '_shape', '_slicetoarange', '_sub_dense', '_sub_sparse', '_swap', '_unpack_index', '_with_data', '_zero_many', 'arcsin', 'arcsinh', 'arctan', 'arctanh', 'argmax', 'argmin', 'asformat', 'asfptype', 'astype', 'ceil', 'check_format', 'conj', 'conjugate', 'copy', 'count_nonzero', 'data', 'deg2rad', 'diagonal', 'dot', 'dtype', 'eliminate_zeros', 'expm1', 'floor', 'format', 'getH', 'get_shape', 'getcol', 'getformat', 'getmaxprint', 'getnnz', 'getrow', 'has_canonical_format', 'has_sorted_indices', 'indices', 'indptr', 'log1p', 'max', 'maximum', 'maxprint', 'mean', 'min', 'minimum', 'multiply', 'ndim', 'nnz', 'nonzero', 'power', 'prune', 'rad2deg', 'reshape', 'resize', 'rint', 'set_shape', 'setdiag', 'shape', 'sign', 'sin', 'sinh', 'sort_indices', 'sorted_indices', 'sqrt', 'sum', 'sum_duplicates', 'tan', 'tanh', 'toarray', 'tobsr', 'tocoo', 'tocsc', 'tocsr', 'todense', 'todia', 'todok', 'tolil', 'transpose', 'trunc']\n"
     ]
    }
   ],
   "source": [
    "testData = ['hey all whatsup', 'dawg not too much']\n",
    "test = CountVectorizer(tokenizer=None, ngram_range=(1,1), max_df=.5, min_df=1)\n",
    "res = test.fit_transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESCR', 'data', 'filenames', 'target', 'target_names']\n"
     ]
    }
   ],
   "source": [
    "print(dir(newsTrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
